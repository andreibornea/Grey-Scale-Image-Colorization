{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faf0hFP6EyOD"
      },
      "source": [
        "#1 - Implementing the model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivraQ5bULb3C"
      },
      "source": [
        "##1.1- Preparing to start\n",
        "The implementation in this project utilizes a training set consisting of 8,000 images from the COCO dataset, which is a smaller subset compared to the extensive ImageNet dataset used in the referenced paper. The training set size in this project accounts for approximately 0.6% of the dataset size used in the paper. It is worth noting that for the colorization task, you can employ various datasets as long as they encompass diverse scenes and locations, which are expected to enable the model to learn colorization effectively. For instance, even a subset of 8,000 images from ImageNet would be sufficient for this particular project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79XVIfb2Kz-U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from fastai.data.external import untar_data, URLs\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_colab = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fceFH79Cx8T"
      },
      "source": [
        "Loading images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xZKQ3R3gW5i"
      },
      "outputs": [],
      "source": [
        "!pip install fastai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNDCPZ0igT02"
      },
      "outputs": [],
      "source": [
        "from fastai.data.external import untar_data\n",
        "coco_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
        "coco_path = untar_data(coco_url)\n",
        "coco_train_path = coco_path / \"train2017\"\n",
        "use_colab = True "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.data.external import untar_data, URLs\n",
        "coco_path = untar_data(URLs.COCO_SAMPLE)\n",
        "coco_path = str(coco_path) + \"/train_sample\"\n",
        "use_colab = True"
      ],
      "metadata": {
        "id": "r5on7H5WjI32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjkppZKfejqi"
      },
      "outputs": [],
      "source": [
        "if use_colab == True:\n",
        "    path = coco_path\n",
        "\n",
        "paths = glob.glob(str(path + \"/*.jpg\")) # Grabbing all the image file names\n",
        "\n",
        "np.random.seed(123)\n",
        "np.random.shuffle(paths)\n",
        "n_images = len(paths)\n",
        "n_train = int(n_images * 0.8) \n",
        "train_paths = paths[:n_train] \n",
        "val_paths = paths[n_train:]\n",
        "\n",
        "print(len(train_paths), len(val_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_I3zDXBfI7N"
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "for ax, img_path in zip(axes.flatten(), train_paths):\n",
        "    ax.imshow(Image.open(img_path))\n",
        "    ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h2bMfjpCly0"
      },
      "source": [
        "##1.2- Making Datasets and DataLoaders (using Data Augumentation)\n",
        "The code implementation incorporates several important techniques for handling and augmenting image data. First, the images undergo resizing and, in the case of the training set, horizontal flipping. These transformations ensure consistency in input dimensions and introduce diversity in the training data by simulating different orientations.\n",
        "\n",
        "Next, a significant step involves converting the RGB images to the Lab color space. This conversion separates the grayscale (L) channel from the color (a, b) channels. The grayscale channel serves as the input to the model, while the color channels act as the target for colorization. This representation enables the model to learn the mapping from grayscale to color.\n",
        "\n",
        "To further enhance the training process, data augmentation is employed. Data augmentation is a powerful technique for increasing the effective size of the training set and improving model generalization. It involves applying various transformations to the images, either during training or as a preprocessing step. In the case of colorization, potential augmentation techniques include rotations, zooms, and horizontal/vertical flips. These augmentations introduce variations and increase the diversity of the training data.\n",
        "\n",
        "In the context of colorizing grayscale images, a particularly useful augmentation is adding noise. By adding random noise to the grayscale images, the model learns to handle and correct small perturbations in the input. This augmentation technique enhances the model's robustness and can contribute to better performance on real-world data.\n",
        "\n",
        "Overall, the code implementation encompasses essential steps such as resizing, flipping, color space conversion, and data augmentation to prepare and augment the image data for effective training of colorization models. These techniques contribute to improved model performance, generalization, and robustness when handling grayscale images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS28CCcjfL_O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "SIZE = 256\n",
        "class ColorizationDatasetAugumentation(Dataset):\n",
        "    def __init__(self, paths, split='train', noise_factor=0.05):\n",
        "        if split == 'train':\n",
        "            self.transforms = transforms.Compose([\n",
        "                transforms.Resize((SIZE, SIZE),  Image.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.Lambda(lambda x: x + noise_factor * torch.randn_like(x))  # Add Gaussian noise\n",
        "            ])\n",
        "        elif split == 'val':\n",
        "            self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n",
        "        \n",
        "        self.split = split\n",
        "        self.size = SIZE\n",
        "        self.paths = paths\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        img = self.transforms(img)\n",
        "        img = np.array(img)\n",
        "        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n",
        "        img_lab = transforms.ToTensor()(img_lab)\n",
        "        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
        "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
        "        \n",
        "        return {'L': L, 'ab': ab}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "def make_dataloaders(batch_size=16, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n",
        "    dataset = ColorizationDataset(**kwargs)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n",
        "                            pin_memory=pin_memory)\n",
        "    return dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#without augumentation\n",
        "SIZE = 256\n",
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, paths, split='train'):\n",
        "        if split == 'train':\n",
        "            self.transforms = transforms.Compose([\n",
        "                transforms.Resize((SIZE, SIZE),  Image.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(), # A little data augmentation!\n",
        "            ])\n",
        "        elif split == 'val':\n",
        "            self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n",
        "        \n",
        "        self.split = split\n",
        "        self.size = SIZE\n",
        "        self.paths = paths\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        img = self.transforms(img)\n",
        "        img = np.array(img)\n",
        "        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n",
        "        img_lab = transforms.ToTensor()(img_lab)\n",
        "        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
        "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
        "        \n",
        "        return {'L': L, 'ab': ab}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "def make_dataloaders(batch_size=16, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n",
        "    dataset = ColorizationDataset(**kwargs)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n",
        "                            pin_memory=pin_memory)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "aSpSobWaF46f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOOgjMbufN_s"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dl = make_dataloaders(paths=train_paths, split='train')\n",
        "val_dl = make_dataloaders(paths=val_paths, split='val')\n",
        "data = next(iter(train_dl))\n",
        "Ls, abs_ = data['L'], data['ab']\n",
        "print(Ls.shape, abs_.shape)\n",
        "print(len(train_dl), len(val_dl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DatldYfNDPqQ"
      },
      "source": [
        "##1.3-Improving Gradient Flow in U-Net: Incorporating Residual Connections\n",
        "Here we implemented a U-Net architecture for image colorization tasks using PyTorch. The U-Net follows a U-shaped architecture, where down-sampling and up-sampling modules are added iteratively around the middle part of the U shape. This design allows for the effective representation of both global and local features in the input data. The U-Net consists of multiple custom module classes, each serving a specific purpose in the overall architecture.\n",
        "\n",
        "The U-Net architecture starts with the ConvLayer and TransposeConvLayer classes, which perform convolutional and transposed convolutional operations, respectively. These layers are responsible for extracting features, capturing spatial information, and upsampling the feature maps to reconstruct the output.\n",
        "\n",
        "The core building block of the U-Net architecture is the UnetBlock class. It represents a single module within the U-Net and plays a crucial role in both the encoding and decoding processes. Each UnetBlock consists of down-convolution, down-normalization, up-relu, up-normalization, and up-convolution operations.\n",
        "\n",
        "During the encoding phase, the down-convolution operation reduces the spatial dimensions and extracts lower-level features from the input data. Batch normalization is then applied for normalization and stability. On the other hand, the up-convolution operation upsamples the feature maps, recovering spatial details and expanding the resolution. The combination of down-convolution and up-convolution operations allows the U-Net to capture both global and local information in a multi-scale manner.\n",
        "\n",
        "To enhance the learning capability of the U-Net, residual connections inspired by the paper \"Deep Residual Learning for Image Recognition\" (He et al., 2015) are incorporated. These connections mitigate the vanishing gradient problem, which can occur when training deeper neural networks. By adding skip connections between corresponding layers of the encoder and decoder, the model can directly transfer learned features, enabling effective gradient propagation and preventing the loss of information.\n",
        "\n",
        "The Unet class represents the complete U-Net architecture. It sequentially stacks multiple UnetBlock instances, creating a comprehensive and deep architecture for image colorization. The U-Net progressively encodes the input data, capturing global and local features, and then decodes it to generate detailed colorization results.\n",
        "\n",
        "Overall, the U-Net architecture, along with the specific operations performed by each layer, facilitates the effective encoding and decoding of features, capturing both global and local information to achieve accurate and detailed colorization of grayscale images. The incorporation of residual connections enhances the learning capability of the model, allowing for better gradient propagation and preservation of intricate details.\n",
        "\n",
        "Reference:\n",
        "\n",
        "- Deep Residual Learning for Image Recognition- https://arxiv.org/pdf/1512.03385.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f32NPMIfQkQ"
      },
      "outputs": [],
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False, activation=None):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        ]\n",
        "        if activation is not None:\n",
        "            layers.append(activation)\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class TransposeConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False, activation=None):\n",
        "        super(TransposeConvLayer, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        ]\n",
        "        if activation is not None:\n",
        "            layers.append(activation)\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UnetBlock(nn.Module):\n",
        "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False, innermost=False, outermost=False):\n",
        "        super(UnetBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if input_c is None:\n",
        "            input_c = nf\n",
        "        downconv = ConvLayer(input_c, ni, activation=nn.LeakyReLU(0.2, inplace=True))\n",
        "        downnorm = nn.BatchNorm2d(ni)\n",
        "        uprelu = nn.ReLU(inplace=True)\n",
        "        upnorm = nn.BatchNorm2d(nf)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = TransposeConvLayer(ni * 2, nf, activation=nn.ReLU(inplace=True))\n",
        "            down = [downconv]\n",
        "            up = [upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = TransposeConvLayer(ni, nf, bias=False)\n",
        "            down = [downconv]\n",
        "            up = [upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = TransposeConvLayer(ni * 2, nf, bias=False)\n",
        "            down = [downconv, downnorm]\n",
        "            up = [upconv, upnorm]\n",
        "            if dropout:\n",
        "                up += [nn.Dropout(0.5)]\n",
        "            model = down + [submodule] + up\n",
        "        self.model = nn.Sequential(*model)\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv2d(input_c, nf, kernel_size=1, stride=1),\n",
        "            nn.BatchNorm2d(nf)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            x_clone = x.clone()\n",
        "            return torch.cat([self.residual(x_clone), self.model(x)], 1)\n",
        "\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
        "        super(Unet, self).__init__()\n",
        "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
        "        for _ in range(n_down - 5):\n",
        "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
        "        out_filters = num_filters * 8\n",
        "        for _ in range(3):\n",
        "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
        "            out_filters //= 2\n",
        "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.tanh(self.model(x)) * 0.5 + 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvD1NfRDDdCb"
      },
      "source": [
        "##1.4- Discriminator (with Dilated Convolutions)\n",
        "Here is the description of how we constructed the discriminator:\n",
        "\n",
        "ConvLayer: The ConvLayer module is used to extract features from the input data. It performs a convolutional operation, followed by instance normalization and a leaky ReLU activation function. The convolution operation captures spatial patterns and important image features. Instance normalization improves the stability and performance of the model by normalizing the features within each instance. The leaky ReLU activation function introduces non-linearity, allowing the discriminator to learn complex and discriminative representations of real and fake images.\n",
        "\n",
        "FinalConvLayer: The FinalConvLayer module is the last convolutional layer in the PatchGAN discriminator. Unlike the ConvLayer, it does not use normalization or an activation function. This layer is responsible for producing the final classification score for determining whether an input image is real or fake. The absence of normalization and activation in this layer ensures that the discriminator's output is not constrained by these operations and can directly influence the classification decision.\n",
        "\n",
        "Dilated Convolutions: Dilated convolutions are used in the PatchGAN discriminator to allow the network to access a wider field of view or larger contextual information without significantly increasing the number of parameters or computational load. By using dilated convolutions, the model can incorporate larger-scale information, which can be particularly beneficial for tasks like image colorization. In image colorization, understanding colors in a larger region can help inform the appropriate colors in a specific location. Additionally, dilated convolutions help preserve the resolution of the images and avoid the loss of fine details that can occur with pooling or strided convolutions. This is crucial for generating high-quality colorized images.\n",
        "\n",
        "Stacked Architecture: The PatchGAN discriminator employs a stacked architecture by sequentially stacking ConvLayer blocks. Each ConvLayer block increases the number of filters and reduces the spatial dimensions of the feature maps. This architecture enables the discriminator to learn hierarchical representations of the input images, capturing both local and global features. The stacking of ConvLayer blocks allows the discriminator to perform multi-scale analysis, capturing discriminative information at different levels of abstraction.\n",
        "\n",
        "In summary, the ConvLayer and FinalConvLayer modules are used to extract features and produce classification scores, respectively, in the PatchGAN discriminator. Dilated convolutions are employed to incorporate larger-scale contextual information and preserve image resolution. The stacked architecture of ConvLayer blocks enables the discriminator to learn hierarchical representations and perform multi-scale analysis of the input images. Together, these features and layers contribute to the PatchGAN discriminator's ability to effectively classify real and fake images for tasks such as image colorization.\n",
        "\n",
        "References:\n",
        "\n",
        "- Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-Image Translation with Conditional Adversarial Networks. https://arxiv.org/pdf/1611.07004.pdf\n",
        "\n",
        "- Yu, F., & Koltun, V. (2016). Multi-Scale Context Aggregation by Dilated Convolutions. https://arxiv.org/pdf/1511.07122.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuIcx-BGfTQd"
      },
      "outputs": [],
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=2, dilation_rate=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1, bias=False, dilation=dilation_rate),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class FinalConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dilation_rate=1):\n",
        "        super(FinalConvLayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1, bias=True, dilation=dilation_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, input_c, num_filters=64, n_down=3, dilation_rate=1):\n",
        "        super(PatchDiscriminator, self).__init__()\n",
        "        model = [ConvLayer(input_c, num_filters, stride=2, dilation_rate=dilation_rate)]\n",
        "        model += [ConvLayer(num_filters * 2 ** i, num_filters * 2 ** (i + 1),\n",
        "                            stride=1 if i == (n_down-1) else 2,\n",
        "                            dilation_rate=dilation_rate)\n",
        "                  for i in range(n_down)]\n",
        "        model += [FinalConvLayer(num_filters * 2 ** n_down, 1, stride=1, dilation_rate=dilation_rate)]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB8G-a_3fVKB"
      },
      "outputs": [],
      "source": [
        "PatchDiscriminator(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfVEZvl1fXR7"
      },
      "outputs": [],
      "source": [
        "discriminator = PatchDiscriminator(3)\n",
        "dummy_input = torch.randn(16, 3, 256, 256) # batch_size, channels, size, size\n",
        "out = discriminator(dummy_input)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pqx2DRcD3Gj"
      },
      "source": [
        "##1.5- GAN Loss (Addressing Color Imbalance in Image Colorization with Weighted Loss Function)\n",
        "\n",
        "The provided class, GANLossWeighted, serves as a convenient tool for calculating the GAN loss in the final model. During initialization, the desired type of loss is determined (e.g., \"vanilla\" in this case), and constant tensors are registered as the \"real\" and \"fake\" labels.\n",
        "\n",
        "When the module is called, it generates an appropriate tensor filled with zeros or ones based on the current stage (real or fake) and computes the loss using the selected loss function.\n",
        "\n",
        "In the context of image colorization tasks, the option to replace the standard cross-entropy loss with the proposed weighted loss in the GAN is significant. Richard Zhang et al., in their paper \"Colorful Image Colorization,\" highlight the problem of color imbalance in traditional approaches. The standard loss calculation often favors desaturated or greyish colors due to the data distribution, while colorful colors are less favored as they are rarer.\n",
        "\n",
        "To address this issue, the proposed weighted loss incorporates class weights based on the prior probabilities of the classes. These weights, derived from the inverse log-probability of the class distribution, are retrieved from the 'prior_probs.npy' file. By assigning more importance to underrepresented (more colorful) classes, the weighted loss helps rebalance the color bias and encourages the model to predict diverse and vibrant colors. As a result, the aesthetic quality of the generated images is improved.\n",
        "\n",
        " Reference:\n",
        "- Zhang, R., Isola, P., Efros, A. A. (2016). Colorful Image Colorization- https://arxiv.org/pdf/1603.08511.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAWCSC5WfaVu"
      },
      "outputs": [],
      "source": [
        "#enhanced loss\n",
        "class GANLossWeighted(nn.Module):\n",
        "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0, num_classes=313, rebalance_factor=0.5):\n",
        "        super().__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "        self.num_classes = num_classes\n",
        "        self.rebalance_factor = rebalance_factor\n",
        "\n",
        "        if gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss(reduction='none')  \n",
        "        elif gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss(reduction='none') \n",
        "\n",
        "        class_distribution = np.load('prior_probs.npy')\n",
        "        class_weights = 1 / (np.log(self.rebalance_factor + class_distribution))\n",
        "        self.register_buffer('class_weights', torch.tensor(class_weights, dtype=torch.float))\n",
        "\n",
        "    def get_labels(self, preds, target_is_real):\n",
        "        if target_is_real:\n",
        "            labels = self.real_label\n",
        "        else:\n",
        "            labels = self.fake_label\n",
        "        return labels.expand_as(preds)\n",
        "    \n",
        "    def __call__(self, preds, target_is_real, use_class_weights=False):\n",
        "        labels = self.get_labels(preds, target_is_real)\n",
        "        losses = self.loss(preds, labels)\n",
        "\n",
        "        if use_class_weights:\n",
        "            weights = self.class_weights[labels.long()]  \n",
        "            weighted_losses = losses * weights  \n",
        "            return weighted_losses.mean()  \n",
        "        else:\n",
        "            return losses.mean()  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stanbdard loss\n",
        "\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "        if gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "    \n",
        "    def get_labels(self, preds, target_is_real):\n",
        "        if target_is_real:\n",
        "            labels = self.real_label\n",
        "        else:\n",
        "            labels = self.fake_label\n",
        "        return labels.expand_as(preds)\n",
        "    \n",
        "    def __call__(self, preds, target_is_real):\n",
        "        labels = self.get_labels(preds, target_is_real)\n",
        "        loss = self.loss(preds, labels)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "xsrtNC74a_3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSzxrWaUFMpx"
      },
      "source": [
        "We are going to initialize the weights of our model with a mean of 0.0 and standard deviation of 0.02 which are the proposed hyperparameters in the article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEWRHtnffcj0"
      },
      "outputs": [],
      "source": [
        "def init_weights(net, init='norm', gain=0.02, print_message=True):\n",
        "    \n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
        "            if init == 'norm':\n",
        "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
        "            elif init == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init == 'kaiming':\n",
        "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "        \n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "        if 'BatchNorm2d' in classname:\n",
        "            nn.init.normal_(m.weight.data, 1., gain)\n",
        "            nn.init.constant_(m.bias.data, 0.)\n",
        "            \n",
        "    net.apply(init_func)\n",
        "    if print_message:\n",
        "        print(f\"model initialized with {init} initialization\")\n",
        "    return net\n",
        "\n",
        "def init_model(model, device, init_type='norm', gain=0.02):\n",
        "    model = model.to(device)\n",
        "    model = init_weights(model, init=init_type, gain=gain)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.6- Attention Layer"
      ],
      "metadata": {
        "id": "DwjAd_x_NptM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code implements a Self-Attention module, which is a mechanism that allows neural networks to focus on different spatial locations within an input. The module consists of query, key, and value convolutional layers, which transform the input feature maps into query, key, and value representations using 1x1 convolutions. These representations are then used to calculate an attention map, which determines the importance of each spatial location. The attended value map is obtained by multiplying the value representation with the transposed attention map. This attended value map is then scaled by a learnable parameter called gamma and added to the original input.\n",
        "\n",
        "Self-Attention is utilized to capture long-range dependencies and enhance the performance of neural networks. By allowing the network to focus on different spatial locations, it can effectively model relationships between distant elements in the input. This is particularly useful in tasks such as image generation and natural language processing, where capturing global dependencies is essential. The Self-Attention module enables the network to attend to relevant regions and selectively incorporate information from those regions, enhancing the model's ability to extract meaningful features and generate high-quality outputs.\n",
        "\n",
        "References:\n",
        "\n",
        "- Show, Attend and Tell: Neural Image Caption\n",
        "Generation with Visual Attention- https://arxiv.org/pdf/1502.03044.pdf \n",
        "- Attention Is All You Need- https://arxiv.org/pdf/1706.03762.pdf."
      ],
      "metadata": {
        "id": "lz_xzwr0OJTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim, activation):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "\n",
        "        # Ensure the output channel of the convolutional layers are at least 1\n",
        "        out_c = max(1, in_dim // 8)\n",
        "\n",
        "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=out_c, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=out_c, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        m_batchsize, C, width, height = x.size()\n",
        "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(m_batchsize, C, width, height)\n",
        "        out = self.gamma*out + x\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "7iR0BnsfNoeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVTxdTxPFU1j"
      },
      "source": [
        "##1.7- Ensambling the model\n",
        "The provided `MainModel` class represents the implementation of a complete model for the task of image colorization. The model architecture and training procedures are designed to achieve effective and high-quality colorization results. \n",
        "\n",
        "The initialization of the `MainModel` class involves setting up the generator (`net_G`) and discriminator (`net_D`) networks. If a generator is not provided, a default Unet model is initialized. The Unet architecture is well-suited for image-to-image translation tasks and consists of an encoder and decoder with skip connections. The Unet generator takes a grayscale image as input and aims to generate the corresponding colorized image. The discriminator is implemented as a PatchGAN discriminator, which focuses on local image patches to distinguish between real and fake images. \n",
        "\n",
        "To facilitate the training process, appropriate learning rates (`lr_G` and `lr_D`), beta coefficients (`beta1` and `beta2`) for the Adam optimizer, and a coefficient for the L1 loss (`lambda_L1`) are defined. The Adam optimizer is commonly used in deep learning models for efficient parameter updates. The L1 loss is employed to ensure that the generated colorization is visually similar to the ground truth colorization.\n",
        "\n",
        "The `forward` method performs the forward pass of the model. It takes the grayscale input image (`L`) and generates the corresponding colorization (`fake_color`) using the generator network (`net_G`). The generated colorization is further enhanced using a self-attention mechanism, which allows the model to focus on important spatial locations during colorization.\n",
        "\n",
        "During the training process, the `backward_D` method trains the discriminator by calculating the GAN loss for both real and fake images. The fake images are obtained by concatenating the grayscale input image (`L`) with the generated colorization (`fake_color`). The real images consist of the grayscale input image (`L`) and the ground truth colorization (`ab`). The discriminator is optimized to distinguish between real and fake images, and the gradients are backpropagated to update the discriminator's parameters.\n",
        "\n",
        "The `backward_G` method trains the generator by calculating the adversarial loss and the L1 loss between the generated colorization and the ground truth colorization. The adversarial loss encourages the generator to produce colorizations that can deceive the discriminator into classifying them as real. The L1 loss measures the pixel-wise difference between the generated colorization and the ground truth, ensuring the generated colorization is visually close to the ground truth. These two losses are combined, with the L1 loss scaled by the `lambda_L1` coefficient, and the gradients are backpropagated to update the generator's parameters.\n",
        "\n",
        "The `optimize` method orchestrates the overall optimization process by calling the forward pass, training the discriminator (`net_D`), training the generator (`net_G`), and updating the optimizer parameters. This iterative training procedure allows the model to learn colorization patterns and adversarial strategies, leading to improved colorization performance.\n",
        "\n",
        "The construction of the model and the training procedures aim to leverage the power of deep learning and adversarial training to generate high-quality colorized images from grayscale inputs. The Unet generator, PatchGAN discriminator, self-attention mechanism, and the utilization of GAN and L1 losses collectively contribute to the model's ability to produce visually pleasing and realistic colorizations.\n",
        "\n",
        "References:\n",
        "- Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-Image Translation with Conditional Adversarial Networks. https://arxiv.org/pdf/1611.07004.pdf\n",
        "\n",
        "- Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative Adversarial Nets. -https://papers.nips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P7Vy37mffnc"
      },
      "outputs": [],
      "source": [
        "class MainModel(nn.Module):\n",
        "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4,\n",
        "                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.lambda_L1 = lambda_L1\n",
        "\n",
        "        if net_G is None:\n",
        "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
        "        else:\n",
        "            self.net_G = net_G.to(self.device)\n",
        "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
        "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
        "        self.L1criterion = nn.L1Loss()\n",
        "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
        "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
        "        self.self_attention = SelfAttention(in_dim=2, activation=nn.ReLU())\n",
        "\n",
        "    def set_requires_grad(self, model, requires_grad=True):\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = requires_grad\n",
        "\n",
        "    def setup_input(self, data):\n",
        "        self.L = data['L'].to(self.device)\n",
        "        self.ab = data['ab'].to(self.device)\n",
        "\n",
        "    def forward(self):\n",
        "        self.fake_color = self.net_G(self.L)\n",
        "        self.fake_color = self.self_attention(self.fake_color)  \n",
        "        self.fake_color = torch.tanh(self.fake_color) * 0.5 + 0.5  \n",
        "\n",
        "    def backward_D(self):\n",
        "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        fake_preds = self.net_D(fake_image.detach())\n",
        "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
        "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
        "        real_preds = self.net_D(real_image)\n",
        "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
        "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "        self.loss_D.backward()\n",
        "\n",
        "    def backward_G(self):\n",
        "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        fake_preds = self.net_D(fake_image)\n",
        "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
        "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
        "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
        "        self.loss_G.backward()\n",
        "\n",
        "    def optimize(self):\n",
        "        self.forward()\n",
        "        self.net_D.train()\n",
        "        self.set_requires_grad(self.net_D, True)\n",
        "        self.opt_D.zero_grad()\n",
        "        self.backward_D()\n",
        "        self.opt_D.step()\n",
        "\n",
        "        self.net_G.train()\n",
        "        self.set_requires_grad(self.net_D, False)\n",
        "        self.opt_G.zero_grad()\n",
        "        self.backward_G()\n",
        "        self.opt_G.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D24sa7RFxVD"
      },
      "source": [
        "The MeasureClass class is a utility class for computing and tracking the average of a value over multiple iterations. It keeps track of the count, sum, and average of the values. The reset method resets the meter, while the update method updates the meter with a new value and count.\n",
        "\n",
        "The create_loss_meters function creates a dictionary of AMeasureClass objects to track different loss values during training. These meters include loss_D_fake, loss_D_real, loss_D, loss_G_GAN, loss_G_L1, and loss_G.\n",
        "\n",
        "The update_losses function updates the loss meters with the corresponding loss values from the model. It takes the model, loss meter dictionary, count of iterations, a SummaryWriter for TensorBoard logging, and the current step as inputs. It iterates through the loss meter dictionary, retrieves the corresponding loss value from the model, updates the corresponding loss meter, and logs the average loss value to TensorBoard using the writer.\n",
        "\n",
        "The lab_to_rgb function converts a batch of Lab* images to RGB images. It takes the L channel (L) and ab channels (ab) as inputs. The function performs the necessary scaling and conversion operations to transform the Lab* images to RGB images.\n",
        "\n",
        "The visualize function visualizes the colorization results of the model. It sets the generator network (net_G) to evaluation mode and generates colorized images from the provided data. It then sets the generator network back to training mode. The function converts the colorized images and the ground truth color images to RGB format using the lab_to_rgb function and visualizes them using matplotlib. The resulting plot shows the grayscale input images, generated colorized images, and ground truth color images side by side.\n",
        "\n",
        "The log_results function logs the average loss values to the console and writes them to TensorBoard using the provided writer. It iterates through the loss meter dictionary, retrieves the average loss value from each meter, prints it to the console, and logs it to TensorBoard.\n",
        "\n",
        "These utility functions play a crucial role in tracking and visualizing the progress of the colorization model during training, allowing for efficient monitoring and analysis of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyk5I7x_fis8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class MeasureClass:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.count, self.avg, self.sum = [0.] * 3\n",
        "    \n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += count * val\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def create_loss_meters():\n",
        "    loss_D_fake = MeasureClass()\n",
        "    loss_D_real = MeasureClass()\n",
        "    loss_D = MeasureClass()\n",
        "    loss_G_GAN = MeasureClass()\n",
        "    loss_G_L1 = MeasureClass()\n",
        "    loss_G = MeasureClass()\n",
        "    \n",
        "    return {'loss_D_fake': loss_D_fake,\n",
        "            'loss_D_real': loss_D_real,\n",
        "            'loss_D': loss_D,\n",
        "            'loss_G_GAN': loss_G_GAN,\n",
        "            'loss_G_L1': loss_G_L1,\n",
        "            'loss_G': loss_G}\n",
        "\n",
        "def update_losses(model, loss_meter_dict, count, writer, step):\n",
        "    for loss_name, loss_meter in loss_meter_dict.items():\n",
        "        loss = getattr(model, loss_name)\n",
        "        loss_meter.update(loss.item(), count=count)\n",
        "        writer.add_scalar(loss_name, loss_meter.avg, step)\n",
        "\n",
        "def lab_to_rgb(L, ab):\n",
        "    \"\"\"\n",
        "    Takes a batch of images\n",
        "    \"\"\"\n",
        "    \n",
        "    L = (L + 1.) * 50.\n",
        "    ab = ab * 110.\n",
        "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "    rgb_imgs = []\n",
        "    for img in Lab:\n",
        "        img_rgb = lab2rgb(img)\n",
        "        rgb_imgs.append(img_rgb)\n",
        "    return np.stack(rgb_imgs, axis=0)\n",
        "    \n",
        "def visualize(model, data, save=True):\n",
        "    model.net_G.eval()\n",
        "    with torch.no_grad():\n",
        "        model.setup_input(data)\n",
        "        model.forward()\n",
        "    model.net_G.train()\n",
        "    fake_color = model.fake_color.detach()\n",
        "    real_color = model.ab\n",
        "    L = model.L\n",
        "    fake_imgs = lab_to_rgb(L, fake_color)\n",
        "    real_imgs = lab_to_rgb(L, real_color)\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "    for i in range(5):\n",
        "        ax = plt.subplot(3, 5, i + 1)\n",
        "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
        "        ax.axis(\"off\")\n",
        "        ax = plt.subplot(3, 5, i + 1 + 5)\n",
        "        ax.imshow(fake_imgs[i])\n",
        "        ax.axis(\"off\")\n",
        "        ax = plt.subplot(3, 5, i + 1 + 10)\n",
        "        ax.imshow(real_imgs[i])\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()\n",
        "    if save:\n",
        "        fig.savefig(f\"colorization_{time.time()}.png\")\n",
        "        \n",
        "def log_results(loss_meter_dict, step, writer):\n",
        "    for loss_name, loss_meter in loss_meter_dict.items():\n",
        "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")\n",
        "        writer.add_scalar(loss_name, loss_meter.avg, step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo8ViIhjF3zl"
      },
      "source": [
        "##1.8- Training function\n",
        "\n",
        "\n",
        "The provided code defines the train_model function, which is responsible for training the MainModel using the specified dataloader (train_dl) for a specified number of epochs. It also includes additional functionalities for logging and visualization.\n",
        "\n",
        "Within the function, a validation data sample is obtained from the validation dataloader (val_dl). A SummaryWriter is created to log the training progress to TensorBoard. The variable step is initialized to keep track of the current step in the training process.\n",
        "\n",
        "The function then enters a loop over the number of epochs. Within each epoch, a new dictionary of loss meters is created using the create_loss_meters function. The variable i is used to keep track of the iteration count.\n",
        "\n",
        "Next, the function iterates through the training dataloader (train_dl) using the tqdm progress bar to provide a visual indication of the training progress. For each batch of data, the model's input is set up, and the optimize method is called to perform optimization and update the model's parameters. The update_losses function is used to update the loss meters with the corresponding loss values from the model, and the losses are logged using the SummaryWriter. The i and step variables are incremented accordingly.\n",
        "\n",
        "At regular intervals determined by the display_every parameter, the function prints the current epoch and iteration information, calls the log_results function to log the average loss values to the console and TensorBoard, and invokes the visualize function to visualize the colorization results of the model using the validation data.\n",
        "\n",
        "After each epoch, the model's state dictionary is saved to a file named with the format \"model_epoch{epoch}_fulldataset.pt\" using torch.save.\n",
        "\n",
        "Once all the epochs are completed, the SummaryWriter is closed, concluding the training process.\n",
        "\n",
        "The last section of the code creates an instance of the MainModel, named model, and calls the train_model function to train the model using the provided dataloader (train_dl) for 100 epochs.\n",
        "\n",
        "These combined functionalities allow for the training of the MainModel, logging of loss values, visualization of results, and saving of model checkpoints at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "MSO3vLOwASm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327cacb0-f0d4-4485-bd63-9bf04496740a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puTVQulTfklb"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dl, epochs, display_every=200):\n",
        "    val_data = next(iter(val_dl))\n",
        "    writer = SummaryWriter()  \n",
        "    step = 0  \n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loss_meter_dict = create_loss_meters()\n",
        "        i = 0\n",
        "\n",
        "        for data in tqdm(train_dl):\n",
        "            model.setup_input(data)\n",
        "            model.optimize()\n",
        "            update_losses(model, loss_meter_dict, count=data['L'].size(0), writer=writer, step=step)\n",
        "            i += 1\n",
        "            step += 1\n",
        "\n",
        "            if i % display_every == 0:\n",
        "                print(f\"\\nEpoch {epoch}/{epochs}\")\n",
        "                print(f\"Iteration {i}/{len(train_dl)}\")\n",
        "                log_results(loss_meter_dict, step, writer)\n",
        "                visualize(model, val_data, save=False)\n",
        "\n",
        "        epoch_filename = f\"model_epoch{epoch}_fulldataset.pt\"\n",
        "        torch.save(model.state_dict(), epoch_filename)\n",
        "        print(f\"Saved weights of epoch {epoch} to {epoch_filename}\")\n",
        "\n",
        "    writer.close() \n",
        "\n",
        "model = MainModel()\n",
        "train_model(model, train_dl, 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDJwIGPTGUnr"
      },
      "source": [
        "#2-Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1- Using a new generator\n",
        "We'll use fastai library's Dynamic U-Net module to easily build a U-Net with a ResNet backbone"
      ],
      "metadata": {
        "id": "h-T7A4EF--gm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5EQjiuPfnS3"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.learner import create_body\n",
        "from torchvision.models.resnet import resnet18\n",
        "from fastai.vision.models.unet import DynamicUnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Le5NX7yfpDL"
      },
      "outputs": [],
      "source": [
        "def build_res_unet(n_input=1, n_output=2, size=256):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    body = create_body(resnet18(pretrained=True), pretrained=True, n_in=n_input, cut=-2)\n",
        "    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
        "    return net_G"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Pretraining the generator for colorization task"
      ],
      "metadata": {
        "id": "Y6azoMqO-6oS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5JHbNtZfsFt"
      },
      "outputs": [],
      "source": [
        "def pretrain_generator(net_G, train_dl, opt, criterion, epochs):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for e in range(epochs):\n",
        "        loss_meter = MeasureClass()\n",
        "        for data in tqdm(train_dl):\n",
        "            L, ab = data['L'].to(device), data['ab'].to(device)\n",
        "            preds = net_G(L)\n",
        "            loss = criterion(preds, ab)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            \n",
        "            loss_meter.update(loss.item(), L.size(0))\n",
        "            \n",
        "        print(f\"Epoch {e + 1}/{epochs}\")\n",
        "        print(f\"L1 Loss: {loss_meter.avg:.5f}\")\n",
        "\n",
        "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
        "opt = optim.Adam(net_G.parameters(), lr=1e-4)\n",
        "criterion = nn.L1Loss()        \n",
        "pretrain_generator(net_G, train_dl, opt, criterion, 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net_G.state_dict(), \"res18-unet.pt\")\n"
      ],
      "metadata": {
        "id": "-DH8jUoiCmbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3- Final training"
      ],
      "metadata": {
        "id": "ClP4EwJSBGyZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz1iofgCft3w"
      },
      "outputs": [],
      "source": [
        "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
        "net_G.load_state_dict(torch.load(f\"res18_unet.pt\", map_location=device))\n",
        "model = MainModel(net_G=net_G)\n",
        "model.load_state_dict(torch.load(\"model_epoch20.pt\", map_location=device))\n",
        "train_model(model, train_dl, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Test the model"
      ],
      "metadata": {
        "id": "QWCaZWeWaUBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
        "net_G.load_state_dict(torch.load(\"res18-unet.pt\", map_location=device))\n",
        "model = MainModel(net_G=net_G)\n",
        "model.load_state_dict(torch.load(\"final_weights.pt\", map_location=device))"
      ],
      "metadata": {
        "id": "GTFqlbiUAh3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from skimage import color\n",
        "\n",
        "def colorize(gray_image_path, model):\n",
        "    gray_image= cv2.imread(gray_image_path)\n",
        "    lab_image = color.rgb2lab(gray_image)\n",
        "    L = lab_image[:,:,0]\n",
        "  \n",
        "    L = (L - 50.) / 100.\n",
        "    L = torch.from_numpy(L).unsqueeze(0).unsqueeze(0).float()\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        L = L.to(model.device)\n",
        "        ab = model.net_G(L).cpu().numpy()\n",
        "    \n",
        "    ab = ab.transpose((0, 2, 3, 1))\n",
        "    ab = ab[0] * 128.\n",
        "    L = (L.cpu().numpy()[0] * 100.) + 50.\n",
        "    L = L[0]\n",
        "    \n",
        "    lab_image = np.concatenate((L[:,:,np.newaxis], ab), axis=2)\n",
        "    colorized_image = color.lab2rgb(lab_image)\n",
        "    \n",
        "    return colorized_image\n",
        "\n",
        "colorized_image= colorize(\"img.jpg\", model)\n",
        "plt.imshow(colorized_image)\n",
        "plt.show()\n",
        "\n",
        "colorized_image_bgr = cv2.cvtColor((colorized_image * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "cv2.imwrite('img.jpg', colorized_image_bgr)"
      ],
      "metadata": {
        "id": "bQ6ul4s-FGLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GdgEJSCsFW4m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}